# Python 常用库介绍（续）

在之前的文档中，我们介绍了一些常用的Python库。这里我们将继续介绍其他有用的库及其示例代码。

## 13. Requests

Requests是一个用于发送HTTP请求的库，简单易用，支持GET、POST等多种请求方式。

```python
import requests  # 导入Requests库

# 发送GET请求
response = requests.get('https://api.github.com')  # 发送GET请求到GitHub API

# 检查响应状态
if response.status_code == 200:  # 检查响应状态码是否为200
    data = response.json()  # 将响应内容解析为JSON格式
    print(data)  # 打印响应数据
else:
    print(f"请求失败，状态码：{response.status_code}")  # 打印错误信息

# 发送POST请求
payload = {'key1': 'value1', 'key2': 'value2'}  # 定义请求数据
response = requests.post('https://httpbin.org/post', data=payload)  # 发送POST请求
print(response.text)  # 打印响应文本
```

## 14. BeautifulSoup

BeautifulSoup是一个用于解析HTML和XML文档的库，常用于网页抓取。

```python
from bs4 import BeautifulSoup  # 从bs4库中导入BeautifulSoup类

# 示例HTML文档
html_doc = """
<html>
    <head><title>The Dormouse's story</title></head>
    <body>
        <p class="title"><b>The Dormouse's story</b></p>
        <p class="story">Once upon a time there were three little sisters; and their names were
            <a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>,
            <a href="http://example.com/lacie" class="sister" id="link2">Lacie</a> and
            <a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>;
            and they lived at the bottom of a well.</p>
    </body>
</html>
"""

# 解析HTML文档
soup = BeautifulSoup(html_doc, 'html.parser')  # 使用HTML解析器解析文档

# 查找所有链接
links = soup.find_all('a')  # 查找所有<a>标签
for link in links:
    print(link.get('href'))  # 打印每个链接的href属性

# 查找特定元素
title = soup.find('title').text  # 查找<title>标签并获取文本
print(title)  # 打印标题文本
```

## 15. Scrapy

Scrapy是一个用于爬取网站数据的框架，支持复杂的爬虫和数据提取。

```python
# Scrapy项目通常在命令行中创建和运行，这里是一个简单的爬虫示例

import scrapy  # 导入Scrapy库

class QuotesSpider(scrapy.Spider):
    name = "quotes"  # 定义爬虫名称

    def start_requests(self):
        urls = [
            'http://quotes.toscrape.com/page/1/',
            'http://quotes.toscrape.com/page/2/',
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)  # 发送请求并指定回调函数

    def parse(self, response):
        for quote in response.css('div.quote'):
            yield {
                'text': quote.css('span.text::text').get(),
                'author': quote.css('span small.author::text').get(),
                'tags': quote.css('div.tags a.tag::text').getall(),
            }
```

## 16. Selenium

Selenium是一个用于自动化Web浏览器操作的库，常用于Web测试和动态内容抓取。

```python
from selenium import webdriver  # 导入Selenium WebDriver

# 创建浏览器对象
driver = webdriver.Chrome()  # 创建Chrome浏览器对象

# 打开网页
driver.get('https://www.example.com')  # 打开指定网址

# 查找元素
element = driver.find_element_by_tag_name('h1')  # 查找h1标签元素
print(element.text)  # 打印元素文本

# 关闭浏览器
driver.quit()  # 关闭浏览器
```

这个文档进一步介绍了一些常用的Python库及其基本用法，特别是用于请求接口和爬取网页的库。希望对你有帮助！ 